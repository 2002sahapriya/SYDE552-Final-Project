{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "72149fd9",
   "metadata": {},
   "source": [
    "# Understanding and Building the Autoencoder\n",
    "\n",
    "Autoencoders are neural networks designed for unsupervised learning tasks, primarily for dimensionality reduction for feature learning. They work by compressing the input into a latent-space representation, and then rescontructing the output from this representation. \n",
    "\n",
    "### How Autoencoders simulate the brain functions:\n",
    "\n",
    "Autoencoders can simulate how certain neurons in the brain might be responsible for detecting specific features such as eyes, nose and mouth by:\n",
    "\n",
    "1. Encoding Phase: Learning a compressed representation of the face, which might be thought of as how the brain encodes visual inputs into a more abstract representation.\n",
    "2. Decoding Phase: Attempting to reconstruct the original image from this compressed representation, similar to how the brain might reconstruct visual information from abstracted signals.\n",
    "\n",
    "### Implementing the autoencoder with VGG-Face\n",
    "1. Feature extraction with VGG-Face\n",
    "2. Building the Decoder\n",
    "3. Training the Autoencoder\n",
    "4. Simulating the neurological deficits\n",
    "\n",
    "We will be using the VGG-Face as the encoder and a mirrored architecture as the decoder. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "604938d0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import required librabries\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import cv2\n",
    "from sklearn.model_selection import train_test_split\n",
    "from keras.preprocessing import image\n",
    "from keras_vggface.vggface import VGGFace\n",
    "from keras.layers import Input, Dense, Flatten, Reshape, Dropout, Conv2DTranspose\n",
    "from keras.models import Model\n",
    "from keras.optimizers import Adam\n",
    "from sklearn.metrics import classification_report\n",
    "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c90d0275",
   "metadata": {},
   "source": [
    "# Step 1: Load and Prepare the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "15b11255",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Helper functions to load and preprocess images\n",
    "def load_images(image_paths, target_size):\n",
    "    images = []\n",
    "    labels= []\n",
    "    for img_path in image_paths:\n",
    "        img = image.load_img(img_path, target_size=target_size)\n",
    "        img = image.img_to_array(img)\n",
    "        img = np.expand_dims(img, axis=0)\n",
    "        img = img / 255.0 # normalize the image pixels\n",
    "        images.append(img)\n",
    "    return np.vstack(images)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "110ad121",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Variables for normal_faces, blurred_eyes, blurred_nose, and blurred_mouth\n",
    "from create-datasets import image_paths, blurred_eyes_paths, blurred_nose_paths, blurred_mouth_path\n",
    "print(len(image_paths))\n",
    "print(len(blurred_eyes_paths))\n",
    "print(len(blurred_nose_paths))\n",
    "print(len(blurred_mouth_paths))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5783adf1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load images \n",
    "X_normal = load_images(image_paths, target_size=(224, 224))\n",
    "y_normal = np.array(labels) # Ensure labels are set correctly as 0 or 1 for binary classification (face/no face)\n",
    "\n",
    "# Split the data\n",
    "X_normal_train, X_normal_test, y_normal_train, y_normal_test = train_test_split(X_normal, y_normal, test_size=0.2, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5d82bb26",
   "metadata": {},
   "outputs": [],
   "source": [
    "# For blurred eyes\n",
    "X_eyes = load_images(blurred_eyes_paths, target_size=(224, 224))\n",
    "y_eyes = np.array(labels)\n",
    "\n",
    "# Split the data\n",
    "X_eyes_train, X_eyes_test, y_eyes_train, y_eyes_test = train_test_split(X_eyes, y_eyes, test_size=0.2, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cb224bf2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# For blurred nose\n",
    "X_nose = load_images(blurred_nose_paths, target_size=(224, 224))\n",
    "y_nose = np.array(labels)\n",
    "\n",
    "# Split the data\n",
    "X_nose_train, X_nose_test, y_nose_train, y_nose_test = train_test_split(X_nose, y_nose, test_size=0.2, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1acd4490",
   "metadata": {},
   "outputs": [],
   "source": [
    "# For blurred mouth\n",
    "X_mouth = load_images(blurred_mouth_paths, target_size=(224, 224))\n",
    "y_mouth = np.array(labels)\n",
    "\n",
    "# Split the data\n",
    "X_mouth_train, X_mouth_test, y_mouth_train, y_mouth_test = train_test_split(X_mouth, y_mouth, test_size=0.2, random_state=42)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "acfb3677",
   "metadata": {},
   "source": [
    "# Step 2: Build the Autoencoder with VGG-Face as Encoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d9fc7641",
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_autoencoder():\n",
    "    # Encoder\n",
    "    vggface = VGGFace(model='vgg16', include_top=False, input_shape=(224, 224, 3), pooling='max')\n",
    "    for layer in vggface.layers:\n",
    "        layer.trainable = False\n",
    "    \n",
    "    # Decoder\n",
    "    flattened = Flatten()(vggface.output)\n",
    "    dense = Dense(512, activation='relu')(flattened)\n",
    "    reshaped = Reshape((8, 8, 8))(dense)\n",
    "    upsample1 = Conv2DTranspose(128, (3, 3), activation='relu', strides=(2, 2), padding='same')(reshaped)\n",
    "    upsample2 = Conv2DTranspose(64, (3, 3), activation='relu', strides=(2, 2), padding='same')(upsample1)\n",
    "    upsample3 = Conv2DTranspose(32, (3, 3), activation='relu', strides=(2, 2), padding='same')(upsample2)\n",
    "    decoder = Conv2DTranspose(3, (3, 3), activation='sigmoid', padding='same')(upsample3)\n",
    "    \n",
    "    # Autoencoder\n",
    "    autoencoder = Model(vggface.input, decoder)\n",
    "    autoencoder.compile(optimizer='adam', loss='mse')\n",
    "    \n",
    "    return autoencoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7ac069f3",
   "metadata": {},
   "outputs": [],
   "source": [
    "autoencoder = build_autoencoder()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4d488ec0",
   "metadata": {},
   "source": [
    "# Step 3: Train the Autoencoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cb9b63c1",
   "metadata": {},
   "outputs": [],
   "source": [
    "autoencoder.fit(X_normal_train,\n",
    "                X_normal_train, \n",
    "                epochs = 10, \n",
    "                batch_size = 32, \n",
    "                validation_data=(X_normal_test, X_normal_test))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c64ec201",
   "metadata": {},
   "source": [
    "# Step 4: Evaluate the model\n",
    "\n",
    "To evaluate the model, we compute accuracy, precision, recall and F1-score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9da564a3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get predictions\n",
    "predictions = autoencoder.predict(X_test)\n",
    "predictions = np.round(predictions).astype(int)\n",
    "\n",
    "# Flatten the predictions and truth for metric calculations\n",
    "y_true = X_test.flatten()\n",
    "y_pred = predictions.flatten()\n",
    "\n",
    "accuracy = accuracy_score(y_true, y_pred)\n",
    "precision = precision_score(y_true, y_pred, average='macro')\n",
    "recall = recall_score(y_true, y_pred, average='macro')\n",
    "f1 = f1_score(y_true, y_pred, average='macro')\n",
    "\n",
    "print(f'Accuracy: {accuracy}\\nPrecision: {precision}\\nRecall: {recall}\\nF1-Score: {f1}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0e7a6381",
   "metadata": {},
   "source": [
    "# Step 5: Simulate Neurological Deficits"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dbdb342a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def deactivate_neurons(layer_name, indices):\n",
    "    layer = autoencoder.get_layer(layer_name)\n",
    "    weights, biases = layer.get_weights()\n",
    "    weights[:, indices] = 0\n",
    "    layer.set_weights([weights, biases])\n",
    "\n",
    "deactivate_neurons('dense_1', [10, 20, 30])  # Example of deactivating certain neurons"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
