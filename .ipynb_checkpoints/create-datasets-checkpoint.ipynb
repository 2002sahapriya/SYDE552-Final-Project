{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "195dded8",
   "metadata": {},
   "source": [
    "## Install the required libraries\n",
    "\n",
    "- keras\n",
    "- dlib\n",
    "- OpenCV\n",
    "- tensorflow\n",
    "- matplotlib"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2168a8dd",
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install dlib opencv-python-headless"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6a4219f4",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import urlib.request\n",
    "from zipfile import ZipFile\n",
    "import cv2\n",
    "import dlib\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f76c8607",
   "metadata": {},
   "source": [
    "# Step 1: Setup and Downloading the Dataset\n",
    "\n",
    "This part involves downloading the LFW dataset and setting up the environment."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "636613c0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to download and extract the dataset\n",
    "def download_and_extract(dataset_url, download_path='datasets/normal_face.zip', extract_path='datasets/normal_faces', remove_file = True):\n",
    "    if not os.path.exists(extract_path):\n",
    "        os.makedirs(os.path.dirname(download_path), exist_ok = True)\n",
    "        print(\"Downloading dataset...\")\n",
    "        urllib.request.urlretrieve(dataset_url, download_path)\n",
    "        print(\"Download complete.\")\n",
    "        \n",
    "        # Extract the zip file\n",
    "        print(\"Extracting dataset...\")\n",
    "        with ZipFile(download_path, 'r') as zip_ref:\n",
    "            zip_ref.extractall(os.path.dirname(extract_path))\n",
    "        print(\"Extraction complete.\")\n",
    "        \n",
    "        # Optionally, remove the zip file\n",
    "        if remove_file:\n",
    "            os.remove(download_path)\n",
    "            print('Zip file removed.')\n",
    "        \n",
    "        print(\"Dataset ready.\")\n",
    "    else:\n",
    "        print(\"Dataset already downloaded and extracted.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "f83ada45",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'download_and_extract' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[1], line 4\u001b[0m\n\u001b[1;32m      2\u001b[0m download \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mTrue\u001b[39;00m\n\u001b[1;32m      3\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m download:\n\u001b[0;32m----> 4\u001b[0m     \u001b[43mdownload_and_extract\u001b[49m()\n\u001b[1;32m      5\u001b[0m     download \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mFalse\u001b[39;00m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'download_and_extract' is not defined"
     ]
    }
   ],
   "source": [
    "# URL of the LFW dataset\n",
    "lfw_url = 'http://vis-www.cs.umass.edu/lfw/lfw.tgz'\n",
    "# Set download to false if the download is not needed\n",
    "download = True\n",
    "if download:\n",
    "    download_and_extract(dataset_url = lfw_url, remove_file = False)\n",
    "    download = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d76fdd6e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to display the first 3 images from the dataset\n",
    "def display_sample_images(base_path, num_images=3):\n",
    "    image_files = [os.path.join(dp, f) for dp, dn, filenames in os.walk(base_path) for f in filenames if os.path.splitext(f)[1].lower() in ['.png', '.jpg', '.jpeg']]\n",
    "    plt.figure(figsize=(15, 5))\n",
    "    for i, file in enumerate(image_files[:num_images]):\n",
    "        image = cv2.imread(file)\n",
    "        image = cv2.cvtColor(image, cv2.COLOR_BGR2RGB)  # Convert from BGR to RGB\n",
    "        plt.subplot(1, num_images, i+1)\n",
    "        plt.imshow(image)\n",
    "        plt.title(f'Image {i+1}')\n",
    "        plt.xticks([])\n",
    "        plt.yticks([])\n",
    "    plt.show()\n",
    "\n",
    "# Display images\n",
    "display_sample_images('datasets/normal_faces')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "97b5f9d7",
   "metadata": {},
   "source": [
    "# Step 2: Setting up Facial Detection and Blurring Features\n",
    "\n",
    "This parts sets up `dlib` for facial detection and defines the blurring function.\n",
    "\n",
    "### Explanation:\n",
    "1. **Mask Creation**: A mask is created to specify which area of the image will be blurred. Initially, the mask is entirely black (`0` values indicate no masking).\n",
    "2. **Convex Hull**: Computes the convex hull of the specified points, which creates a minimal convex boundary that encloses all the points. This hull is used to define the area on the mask that will correspond to the facial feature to blur.\n",
    "3. **Blurring and Masking**: First, we apply a Gaussian blur to the entire image. It then uses the mask to combine the blurred version of the feature area with the original image. The mask ensures that only the specified feature region is blurred, leaving the rest of the image unaffected.\n",
    "4. **Bitwise Operations**: `cv2.bitwise-and` and `cv2.bitwise_not` are used to isolate and the merge the blurred and non-blurred parts of the image. This approach ensures that the blurring effect is cleanly applied only to the desired facial feature."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b609660a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the pre-trained face detector and facial landmark predictor \n",
    "dector = dlib.get_frontal_face_detector()\n",
    "# Path to the facial landmark predictor\n",
    "predictor_path = 'shape_predictor_68_face_landmarks.dat' # have to download this file\n",
    "# Load the facial landmark predictor\n",
    "predictor = dlib.shape_predictor(predictor_path)\n",
    "\n",
    "def blur_feature(image, landmarks, feature_indices):\n",
    "    '''\n",
    "    Applies a blurring effect to specific features on the face identified by landmark indices.\n",
    "    \n",
    "    Parameters:\n",
    "    image(numpy.ndarray): The original image\n",
    "    landmarks (dlib.full_object_detection): Facial landmarks detected by dlib predictor.\n",
    "    feature_indices (list of int): Indices of the landmarks that define the region to blur.\n",
    "    \n",
    "    Returns:\n",
    "    numpy.ndarray: The image with the specified feature blurred. \n",
    "    '''\n",
    "    \n",
    "    # Create a black mask with the same dimentions as the image\n",
    "    mask = np.zeros_like(image)\n",
    "    \n",
    "    # Collect points from the landmarks based on the provided indices\n",
    "    points = np.array([(landmarks.part(n).x, landmarks.part(n).y) for n in feature_indices], np.int32)\n",
    "    \n",
    "    # Create a convex hull around the feature points (the smallest convex shape that includes all points)\n",
    "    convex_hull = cv2.convexHull(points)\n",
    "    \n",
    "    # Fill the convex hull on the mask to define the region to blur\n",
    "    cv2.fillConvexPoly(mask, convex_hull, 255)\n",
    "    \n",
    "    # Apply Gaussian blur to the whole image\n",
    "    image_blurred = cv2.GaussianBlur(image, (99, 99), 30)\n",
    "    \n",
    "    # Combine the blurred image and the mask to blur only the feature area\n",
    "    image_blurred_with_mask = cv2.bitwise_and(image_blurred, mask)\n",
    "    \n",
    "    # Use bitwise operations to isolate the non-blurred part of the original image\n",
    "    final_image = cv2.bitwise_and(image, cv2.bitwise_not(mask))\n",
    "    \n",
    "    # Add the blurred feature area back to the non-blurred image part\n",
    "    \n",
    "    return final_image"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "948a66f7",
   "metadata": {},
   "source": [
    "# Step 3: Processing Images to Create Modified Datasets\n",
    "\n",
    "This part processes each image to blur specific facial features."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0bb75370",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Directory of the dataset and output directories\n",
    "data_dir = 'datasets/normal_faces' # Path where the original dataset is stored\n",
    "output_dirs = {\n",
    "    'eyes': 'datasets/blurred_eyes', # Directory to store images with blurred eyes\n",
    "    'nose': 'datasets/burred_nose',  # Directory to store images with blurred nose\n",
    "    'mouth': 'datasets/blurred_mouth' # Director to store images with blurred mouth\n",
    "}\n",
    "\n",
    "# Create the output directories if they do not exist\n",
    "for path in output_dirs.values():\n",
    "    os.makedirs(path, exist_ok=True)\n",
    "\n",
    "# Define the landmark indices for each facial feature based on the 68-point model\n",
    "feature_indices = {\n",
    "    'eyes': list(range(36, 42)) + list(range(42, 48)), # Indices for left and right eyes\n",
    "    'nose': list(range(27, 36)) # Indices for the nose\n",
    "    'mouth': list(range(48, 68)) # Indices for the mouth\n",
    "}\n",
    "\n",
    "# Walk through the dataset director and process each image\n",
    "for root, dirs, files in os.walk(data_dir):\n",
    "    for file in files:\n",
    "        if file.lower().endswith(('png', 'jpg', 'jpeg')): # Filter to process only image files\n",
    "            file_path = os.path.join(root, file) # construct the full file path\n",
    "            image = cv2.imread(file_path) # Read the image\n",
    "            gray = cv2.cvtColor(image, cv2.COLOR_BGR2GRAY) # Convert the image to grayscale\n",
    "            \n",
    "            # Detect faces in the image\n",
    "            faces = detector(gray) \n",
    "            for face in faces:\n",
    "                landmarks = predictor(gray, face) # Detect landmarks for each face\n",
    "                \n",
    "                # Process each defined features (eyes, nose, mouth)\n",
    "                for features, indices in feature_indices.items():\n",
    "                    # Blur the specific features on the image\n",
    "                    modified_image = blur_features(image.copy(), landmarks, indices)\n",
    "                    # Define the output path for the blurred image\n",
    "                    output_image = os.path.join(output_dirs[feature], file)\n",
    "                    # Save the modified image to the appropriate directory\n",
    "                    cv2.imwrite(output_path, modified_image)\n",
    "\n",
    "print(\"Modified datasets created.\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
